{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"GAN Mini-Project: Monet-Style Image Generation\n\n\n## Introduction\n\nI decided to work on the Kaggle 'I’m Something of a Painter Myself\" competition this week. The main goal is to build a simple GAN that can take a photo and output a Monet-style painting.  \n\n**Problem:**  \n* Build a generative model GAN to translate between “photo” and “Monet painting” domains.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"####  Dataset Description\n* The dataset contains four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. The monet_tfrec and monet_jpg directories contain the same painting images, and the photo_tfrec and photo_jpg directories contain the same photos.\n\n### Files\n- monet_jpg - 300 Monet paintings sized 256x256 in JPEG format\n- monet_tfrec - 300 Monet paintings sized 256x256 in TFRecord format\n- photo_jpg - 7028 photos sized 256x256 in JPEG format\n- photo_tfrec - 7028 photos sized 256x256 in TFRecord format\n","metadata":{}},{"cell_type":"markdown","source":"* I analyzed these notebooks: https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial --- https://www.kaggle.com/code/dimitreoliveira/introduction-to-cyclegan-monet-paintings","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kaggle_datasets import KaggleDatasets\nimport glob","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path()\nMONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'Monet TFRecord files: {len(MONET_FILENAMES)}')\nprint(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### How many image we have","metadata":{}},{"cell_type":"code","source":"monet_folder = '/kaggle/input/gan-getting-started/monet_jpg'\nphoto_folder = '/kaggle/input/gan-getting-started/photo_jpg/'\n\nmonet_files = glob.glob(os.path.join(monet_folder, '*.jpg'))\nphoto_files = glob.glob(os.path.join(photo_folder, '*.jpg'))\n\nprint(f\"Number of Monet images: {len(monet_files)}\")\nprint(f\"Number of Photo images: {len(photo_files)}\")  \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"\n\n#this function loads random images\ndef load_random_image(folder, size=(256, 256)):\n    fname = random.choice(os.listdir(folder))\n    img = Image.open(os.path.join(folder, fname)).resize(size)\n    return np.array(img)\n\nplt.figure(figsize=(6, 6))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    img = load_random_image(monet_folder)\n    plt.imshow(img)\n    plt.axis('off')\nplt.suptitle(\"Random 9 Monet Paintings\")\nplt.show()\n\nplt.figure(figsize=(6, 6))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    img = load_random_image(photo_folder)\n    plt.imshow(img)\n    plt.axis('off')\nplt.suptitle(\"Random 9 Photos\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef pixel_histogram(folder, num_samples=50):\n    all_pixels = []\n    samples = random.sample(os.listdir(folder), num_samples)\n    for fname in samples:\n        img = np.array(Image.open(os.path.join(folder, fname)).resize((256, 256)))\n       \n        all_pixels.extend(img.flatten())\n    return np.array(all_pixels)\n\nmonet_pixels = pixel_histogram(monet_folder, num_samples=50)\nphoto_pixels = pixel_histogram(photo_folder, num_samples=50)\n\nplt.figure(figsize=(8, 4))\nplt.hist(monet_pixels, bins=50, alpha=0.5, label='Monet')\nplt.hist(photo_pixels, bins=50, alpha=0.5, label='Photo')\nplt.xlabel(\"Pixel intensity\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.title(\"Pixel Intensity Distribution: Monet vs. Photo\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**EDA Comments:**  \n- All images are 256×256×3.  \n- The Monet pixel histogram is slightly narrower compared to photos.  \n- No obvious corrupted files.  \n- Colors are very diverse in these examples","metadata":{}},{"cell_type":"markdown","source":"## Model Architecture\n\n- I’m not trying to beat state-of-the-art; I just want to show a working GAN.\n- encoder-decoder style with same spatial dimensions, the reason is we want to preserve photo structure while changing style\n* **Generator**:\n     - Uses encoder-decoder style\n     - 6 Conv2D layers gradually learn style transformation\"\n- **Discriminator**:\n    - Progressive downsampling: 256,12,864,32,16 output a single probability (real/fake).\n    - LeakyReLU to avoid dead neurons\n","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = [256, 256]\nBATCH_SIZE = 1\nEPOCHS = 50","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Code for Generator","metadata":{}},{"cell_type":"code","source":"def build_photo_to_monet_generator():\n   \n    model = tf.keras.Sequential(name=\"Photo_to_Monet_Generator\")\n    \n    \n    model.add(tf.keras.layers.Conv2D(64, (7,7), strides=(1,1), padding='same', \n                                     input_shape=[256, 256, 3]))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    \n    \n    model.add(tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    \n    model.add(tf.keras.layers.Conv2D(256, (3,3), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    \n    \n    for _ in range(6):  # ResNet-style blocks\n        x = model.output if hasattr(model, 'output') else None\n        model.add(tf.keras.layers.Conv2D(256, (3,3), strides=(1,1), padding='same'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.ReLU())\n        model.add(tf.keras.layers.Conv2D(256, (3,3), strides=(1,1), padding='same'))\n        model.add(tf.keras.layers.BatchNormalization())\n    \n    \n    model.add(tf.keras.layers.Conv2DTranspose(128, (3,3), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    \n    model.add(tf.keras.layers.Conv2DTranspose(64, (3,3), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    \n    \n    model.add(tf.keras.layers.Conv2D(3, (7,7), strides=(1,1), padding='same', activation='tanh'))\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_simple_photo_to_monet_generator():\n    \n    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n    \n    # Apply artistic transformations\n    x = tf.keras.layers.Conv2D(32, 3, padding='same')(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    \n    x = tf.keras.layers.Conv2D(64, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    \n    x = tf.keras.layers.Conv2D(128, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    \n    # Style transformation layers\n    x = tf.keras.layers.Conv2D(128, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    \n    x = tf.keras.layers.Conv2D(64, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    \n    x = tf.keras.layers.Conv2D(32, 3, padding='same')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    \n    # Output (Monet-style image)\n    outputs = tf.keras.layers.Conv2D(3, 3, padding='same', activation='tanh')(x)\n    \n    return tf.keras.Model(inputs=inputs, outputs=outputs, name=\"Simple_Photo_to_Monet\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Code for Discriminator","metadata":{}},{"cell_type":"code","source":"def build_discriminator():\n    \n    model = tf.keras.Sequential(name=\"Monet_Discriminator\")\n    \n    model.add(tf.keras.layers.Conv2D(64, (4,4), strides=(2,2), padding='same', \n                                     input_shape=[256, 256, 3]))\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    \n    model.add(tf.keras.layers.Conv2D(128, (4,4), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    \n    model.add(tf.keras.layers.Conv2D(256, (4,4), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    \n    model.add(tf.keras.layers.Conv2D(512, (4,4), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    \n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Comments\n* Input: 256×256×3\n\n*  Flatten & Dense to single output\n\n* Used sigmoid to output probability real/fake.","metadata":{}},{"cell_type":"code","source":"generator = build_photo_to_monet_generator()\ndiscriminator = build_discriminator()\n\n\ngen_optimizer = tf.keras.optimizers.Adam(0.001)\ndisc_optimizer = tf.keras.optimizers.Adam(0.001)\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef train_step_photo_to_monet(real_photos, real_monet):\n    \n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        #generate fake Monet from real photos\n        fake_monet = generator(real_photos, training=True)\n        \n        #discriminator outputs\n        real_monet_output = discriminator(real_monet, training=True)\n        fake_monet_output = discriminator(fake_monet, training=True)\n        \n       \n        gen_loss = cross_entropy(tf.ones_like(fake_monet_output), fake_monet_output)\n        \n       \n        disc_real_loss = cross_entropy(tf.ones_like(real_monet_output), real_monet_output)\n        disc_fake_loss = cross_entropy(tf.zeros_like(fake_monet_output), fake_monet_output)\n        disc_loss = (disc_real_loss + disc_fake_loss) * 0.5\n        \n    # gradients\n    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n    \n    return gen_loss, disc_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_photo_to_monet(photo_dataset, monet_dataset, epochs):\n    \n    for epoch in range(epochs):\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        epoch_gen_loss = []\n        epoch_disc_loss = []\n        \n        \n        combined_ds = tf.data.Dataset.zip((photo_dataset, monet_dataset))\n        \n        for batch_num, (photo_batch, monet_batch) in enumerate(combined_ds):\n            gen_loss, disc_loss = train_step_photo_to_monet(photo_batch, monet_batch)\n            epoch_gen_loss.append(gen_loss)\n            epoch_disc_loss.append(disc_loss)\n            \n            if batch_num % 20 == 0:\n                print(f\"  Batch {batch_num}: gen_loss={gen_loss:.4f}, disc_loss={disc_loss:.4f}\")\n        \n       \n        avg_gen_loss = tf.reduce_mean(epoch_gen_loss)\n        avg_disc_loss = tf.reduce_mean(epoch_disc_loss)\n        print(f\"  → Epoch avg: gen_loss={avg_gen_loss:.4f}, disc_loss={avg_disc_loss:.4f}\")\n        \n        \n        if (epoch + 1) % 2 == 0:\n            show_transformations(epoch + 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_transformations(epoch):\n    \n    fig, axes = plt.subplots(3, 2, figsize=(10, 12))\n    \n    for i, photo_batch in enumerate(photo_ds.take(3)):\n        photo = photo_batch[0]\n        transformed = generator(photo_batch, training=False)[0]\n        \n        # Convert for display\n        photo_display = (photo * 0.5 + 0.5).numpy()\n        transformed_display = (transformed * 0.5 + 0.5).numpy()\n        \n        axes[i, 0].imshow(photo_display)\n        axes[i, 0].set_title(\"Original Photo\")\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(transformed_display)\n        axes[i, 1].set_title(\"Generated Monet \")\n        axes[i, 1].axis('off')\n    \n    plt.suptitle(f\"Photo-to-Monet Transformations - Epoch {epoch}\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = build_simple_photo_to_monet_generator()\ndiscriminator = build_discriminator()\ngen_optimizer = tf.keras.optimizers.Adam(0.002, beta_1=0.5)  \ndisc_optimizer = tf.keras.optimizers.Adam(0.002, beta_1=0.5)\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ngenerator.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing\n* I didn’t use any data augmentation in this version because the training already takes a while and I wanted to keep it simple for now.\n* Normalization: Converting pixel values from [0,255] to [-1,1] reason: Tanh activation in generator outputs [-1,1].\n* No data cleaning needed.","metadata":{}},{"cell_type":"code","source":"def load_and_preprocess(path):\n    \n    img = Image.open(path).convert('RGB').resize((256, 256))\n    arr = np.asarray(img).astype(np.float32)\n    arr = (arr / 127.5) - 1.0  \n    return arr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"monet_images = []\nfor i, path in enumerate(monet_files):\n    if i % 50 == 0:\n        print(f\"Loading Monet image {i+1}/{len(monet_files)}\")\n    monet_images.append(load_and_preprocess(path))\nphoto_images = []\n\nfor i, path in enumerate(photo_files):  \n    if i % 100 == 0:\n        print(f\"Loading photo {i+1}/1000\")\n    photo_images.append(load_and_preprocess(path))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"monet_ds = tf.data.Dataset.from_tensor_slices(monet_images).batch(1)\nphoto_ds = tf.data.Dataset.from_tensor_slices(photo_images).batch(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_photo_to_monet(photo_ds, monet_ds, epochs=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, ax = plt.subplots(5, 2, figsize=(12, 12))\nfor i, img in enumerate(photo_ds.take(5)):\n    prediction = generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(img)\n    ax[i, 1].imshow(prediction)\n    ax[i, 0].set_title(\"Original Photo\")\n    ax[i, 1].set_title(\"Monet Style\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nThis mini-project helped me understand how a GAN works, especially interaction between the generator and discriminator. My final model could generate Monet-style images that at least looked they were able get the shape and little bit style. \n\nWhat worked:\n- Using batch size = 1 helped with limited compute.\n- Visual outputs every 2 epochs helped me track progress.\n\nWhat didn’t work:\n- Generator loss sometimes dropped to 0 or exploded, which probably means more tuning is needed.\n\nFuture ideas:\n- Adding image augmentation and running longer would work.","metadata":{}},{"cell_type":"code","source":"import PIL\nos.makedirs(\"../images\", exist_ok=True)\n\n# Transform all photos and save them\nprint(\"Creating submission images...\")\ni = 1\nfor img in photo_ds:\n    prediction = generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1\n\nprint(f\"Made {i-1} images\")\n\nimport shutil\n\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T18:15:53.831784Z","iopub.execute_input":"2025-06-05T18:15:53.832012Z","iopub.status.idle":"2025-06-05T18:15:53.931221Z","shell.execute_reply.started":"2025-06-05T18:15:53.831992Z","shell.execute_reply":"2025-06-05T18:15:53.929291Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1008130803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Transform all photos and save them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating submission images...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"],"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}